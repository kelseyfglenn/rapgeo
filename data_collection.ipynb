{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597997505634",
   "display_name": "Python 3.7.7 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pickle as pkl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Here you can load the resulting Data from these API requests and scrapes. A Genius API access token is required to run the request code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load artist names.\n",
    "\"\"\"\n",
    "with open('data/artist_names.pkl', 'rb') as f: \n",
    "    artists = pkl.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load artist API paths. \n",
    "\"\"\"\n",
    "with open('data/all_artist_paths.pkl', 'rb') as f:\n",
    "    all_artist_paths = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load song paths for every artist. \n",
    "\"\"\"\n",
    "with open('data/all_song_paths.pkl', 'rb') as f: \n",
    "    all_song_paths = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load all song lyrics.\n",
    "\"\"\"\n",
    "with open('data/all_song_lyrics.pkl', 'rb') as f: \n",
    "    all_song_lyrics = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Request and Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base genius API endpoint\n",
    "url_api = \"https://api.genius.com\"\n",
    "\n",
    "# access token for API requests\n",
    "# you will need to obtain your own access token from the Genius API to run these requests\n",
    "with open('client_access_token.txt', 'r') as f:\n",
    "    client_access_token = f.read()\n",
    "\n",
    "# headers for API requests\n",
    "headers = {\"Authorization\": \"Bearer \" + client_access_token, \"User-Agent\":\"\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artist_profile(artist_name):\n",
    "    \"\"\"\n",
    "    get entire API json object for an artist\n",
    "    \"\"\"\n",
    "    url_search = \"/search?q=\" \n",
    "    querystring = url_api + url_search + quote(artist_name)\n",
    "    # get API response\n",
    "    response = requests.get(querystring, headers=headers)\n",
    "    response_artist = response.json()\n",
    "    return response_artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artist_path(artist_name):\n",
    "    \"\"\"\n",
    "    get API path for an artist\n",
    "    \"\"\"\n",
    "    # generate and store url, modify artist name to remove spaces\n",
    "    url_search = \"/search?q=\" \n",
    "    querystring = url_api + url_search + quote(artist_name)\n",
    "    # get API response\n",
    "    response = requests.get(querystring, headers=headers)\n",
    "    response_artist = response.json()\n",
    "    # pull artist url -- assumes they are the primary artist in the first search result since all searches return song objects\n",
    "    url_artist = response_artist['response']['hits'][0]['result']['primary_artist']['api_path']\n",
    "\n",
    "    return url_artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artist_url(artist_name):\n",
    "    \"\"\"\n",
    "    get genius.com url for an artist \n",
    "    \"\"\"\n",
    "    # generate and store url, modify artist name to remove spaces\n",
    "    url_search = \"/search?q=\" \n",
    "    querystring = url_api + url_search + quote(artist_name)\n",
    "    # get API response\n",
    "    response = requests.get(querystring, headers=headers)\n",
    "    response_artist = response.json()\n",
    "    # pull artist url -- assumes they are the primary artist in the first search result since all searches return song objects\n",
    "    url_artist = response_artist['response']['hits'][0]['result']['primary_artist']['url']\n",
    "\n",
    "    return url_artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_list(url_artist):\n",
    "    \"\"\"\n",
    "    get the first 50 song objects for an artist\n",
    "    **it should be noted that the Genius API lists essentially all content as a \"song\" object,\n",
    "    meaning that this will return interview transcripts and other non-song content that must dealt with in processing\n",
    "    \"\"\"\n",
    "    # get the first per_page songs returned for that artist\n",
    "    per_page = 50\n",
    "    querystring = url_api + url_artist + \"/songs\" + \"?per_page=\" + str(per_page)\n",
    "    response_songs = requests.get(querystring, headers = headers)\n",
    "    songs = response_songs.json()\n",
    "\n",
    "    # reduce songs to only those where target artist is the primary artist\n",
    "    drops = []\n",
    "    for index, song in enumerate(songs['response']['songs']):\n",
    "        if song['primary_artist']['api_path'] != url_artist:\n",
    "            drops.append(index)\n",
    "    for index in sorted(drops, reverse=True):\n",
    "        del songs['response']['songs'][index]\n",
    "\n",
    "    return songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_paths(songs):\n",
    "    \"\"\"\n",
    "    get API path for individual songs\n",
    "    works (most of the time) as a genius.com URL for the song\n",
    "    \"\"\"\n",
    "    song_paths = []\n",
    "    for song in songs['response']['songs']: \n",
    "        song_paths.append(song['api_path'])\n",
    "    return song_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics(song_path):\n",
    "    \"\"\"\n",
    "    get song lyrics for a given song URL\n",
    "    \"\"\"\n",
    "    url = 'https://genius.com' + song_path\n",
    "    lyrics_page = requests.get(url).text\n",
    "    lyrics_soup = BeautifulSoup(lyrics_page)\n",
    "    lyrics = lyrics_soup.find_all('div', class_='lyrics')[0].find('p').text\n",
    "    return lyrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping and API Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nScraping/cleaning/storing artist names from Wikipedia.\\n'"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "\"\"\"\n",
    "Scraping/cleaning/storing artist names from Wikipedia.\n",
    "\"\"\"\n",
    "\n",
    "# # Scrape ~1400 popular hip hop musicians from wikipedia\n",
    "# response = requests.get('https://en.wikipedia.org/wiki/List_of_hip_hop_musicians').text\n",
    "# soup = BeautifulSoup(response)\n",
    "\n",
    "# artist_tags = []\n",
    "# for item in soup.find_all('li'): \n",
    "#     artist_tags.append(item.text)\n",
    "\n",
    "# artists = artist_tags[29:-61]\n",
    "\n",
    "# # remove annotation links (e.g. 'Drake[1]' -> 'Drake')\n",
    "# # remove parenthesized annotations\n",
    "# annotations_list = set([artist for artist in artists if artist[-1] == ']'])\n",
    "# drops = []\n",
    "# for index, artist in enumerate(artists):\n",
    "#     if artist in annotations_list: \n",
    "#         artists[index] = artist[:-3]\n",
    "#     if artist == 'Torch (American)': \n",
    "#         artists[index] = 'Torch (Triple C)'\n",
    "#     if artist == 'Torch (German)':\n",
    "#         artists[index] = 'Torch'\n",
    "#     if artist == 'Casanova (rapper)':\n",
    "#         artists[index] = 'Casanova'\n",
    "#     if artist == 'Alias (musician)' or artist == 'Juice (ƒêus)':\n",
    "#         drops.append(index)\n",
    "\n",
    "# for index in sorted(drops, reverse=True): \n",
    "#     del artists[index]\n",
    "\n",
    "# # Store as pickle\n",
    "# with open('artist_names.pkl', 'rb') as f: \n",
    "#     pkl.dump(artists, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pull and store artist URL's\n",
    "\n",
    "**note: this didn't check for primary_artist so it didn't pull properly for anyone who's not the primary artist on their first searched song\n",
    "e.g. a bunch of people's names will link to Kanye\n",
    "\"\"\"\n",
    "# all_artist_urls = {}\n",
    "# for artist in artists:\n",
    "#     try:\n",
    "#         all_artist_urls[artist] = get_artist_url(artist)\n",
    "#     except:\n",
    "#         all_artist_urls[artist] = None    \n",
    "\n",
    "# with open('all_artist_urls.pkl', 'wb') as f:\n",
    "#     pkl.dump(all_artist_urls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nPull and store artist API paths. \\n'"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pull and store artist API paths. \n",
    "\"\"\"\n",
    "# # Pull artist API path for each artist in list\n",
    "# all_artist_paths = {}\n",
    "# for artist in artists:\n",
    "#     try:\n",
    "#         all_artist_paths[artist] = get_artist_path(artist)\n",
    "#     except:\n",
    "#         all_artist_paths[artist] = None    \n",
    "#\n",
    "# with open('all_artist_paths.pkl', 'wb') as f:\n",
    "#     pkl.dump(all_artist_paths, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nPull and store song paths by artist. \\n'"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pull and store song paths by artist. \n",
    "\"\"\"\n",
    "# all_song_paths = {}\n",
    "# for artist in artists:\n",
    "#     path = all_artist_paths[artist]\n",
    "#     try: \n",
    "#         songs = get_song_list(path)\n",
    "#         all_song_paths[artist] = get_song_paths(songs)\n",
    "#     except:\n",
    "#         all_song_paths[artist] = None\n",
    "\n",
    "# with open('all_song_paths.pkl', 'wb') as f:\n",
    "#     pkl.dump(all_song_paths, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nScrape and store lyrics for each song\\n'"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "\"\"\"\n",
    "Scrape and store lyrics for each song\n",
    "\"\"\"\n",
    "# all_song_lyrics = {}\n",
    "# for artist in artists:\n",
    "#     all_song_lyrics[artist] = {}\n",
    "#     try:       \n",
    "#         for song in all_song_paths[artist]:\n",
    "#             lyrics = get_lyrics(song)\n",
    "#             all_song_lyrics[artist][song] = lyrics\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "# with open('all_song_lyrics.pkl', 'wb') as f:\n",
    "#     pkl.dump(all_song_lyrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Identify artists who we failed to pull lyrics for\n",
    "\"\"\"\n",
    "skipped_artists = [artist for artist in all_song_lyrics.keys() if all_song_lyrics[artist] == {}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Try scraping again in case it was due to some request error\n",
    "\n",
    "**Preloaded data has already run this so it will throw an error. Uncommment for use on a fresh scrape.\n",
    "\"\"\"\n",
    "# skipped_song_lyrics = {}\n",
    "# for artist in skipped_artists:\n",
    "#     skipped_song_lyrics[artist] = {}  \n",
    "#     for song in all_song_paths[artist]:\n",
    "#         try:\n",
    "#             lyrics = get_lyrics(song)\n",
    "#             skipped_song_lyrics[artist][song] = lyrics\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "# all_song_lyrics_v2 = all_song_lyrics.copy()\n",
    "# for artist in skipped_song_lyrics.keys(): \n",
    "#     all_song_lyrics_v2[artist] = skipped_song_lyrics[artist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store our lyrics and head over to topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/all_song_lyrics_v2.pkl', 'wb') as f: \n",
    "    pkl.dump(all_song_lyrics_v2, f)"
   ]
  }
 ]
}