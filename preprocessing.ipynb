{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597997507700",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module and Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Most of these imports are unused, but were used in experimentation during cleaning and prosodic feature engineering.\n",
    "\"\"\"\n",
    "import pickle as pkl\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import syllables\n",
    "import cmudict as cmu\n",
    "from langdetect import detect, detect_langs\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load artist names.\n",
    "\"\"\"\n",
    "with open('data/artist_names.pkl', 'rb') as f: \n",
    "    artists = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load artist API paths. \n",
    "\"\"\"\n",
    "with open('data/all_artist_paths.pkl', 'rb') as f:\n",
    "    all_artist_paths = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load song paths for every artist. \n",
    "\"\"\"\n",
    "with open('data/all_song_paths.pkl', 'rb') as f: \n",
    "    all_song_paths = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load all song lyrics.\n",
    "\"\"\"\n",
    "with open('data/all_song_lyrics_v2.pkl', 'rb') as f: \n",
    "    all_song_lyrics = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning & Feature Engineering\n",
    "This notebook focuses on extracting vocabulary and prosodic information, rather than straightforward lemmatization and tokenization. \n",
    "\n",
    "Due to the significance of slang, interjections and repetition in lyrics as opposed to prose, we choose to retain what would generally be irrelevant or noisy features of the text for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CONVERT FROM LYRICS TO STANDARD TEXT\n",
    "\n",
    "Rebuild the lyrics dict with cleaned, confirmed songs: \n",
    "1) Confirm the artist has a song\n",
    "2) Confirm the lyrics are to a song (not an interview transcript, etc)\n",
    "3) Confirm it is at least 16 lines (1+ standard verses)\n",
    "4) Replace bracketed text denoting verse/chorus separation\n",
    "\"\"\"\n",
    "cleaned_lyrics = {}\n",
    "for artist in all_song_lyrics.keys():\n",
    "    \n",
    "    # confirm artist has a song\n",
    "    if all_song_lyrics[artist] != {}:\n",
    "        \n",
    "        # add the artist and create a dict for their songs\n",
    "        cleaned_lyrics[artist] = {}\n",
    "        for song in all_song_lyrics[artist].keys():\n",
    "            is_song = False\n",
    "            is_sixteen = False\n",
    "            # if there is a verse or chorus marker, add the song to the artist's dict\n",
    "            if '[Verse' in all_song_lyrics[artist][song] or '[Chorus' in all_song_lyrics[artist][song]:\n",
    "                is_song = True\n",
    "            # if the song has at least 16 lines (standard verse)\n",
    "            if all_song_lyrics[artist][song].count('\\n') > 14:   \n",
    "                is_sixteen = True\n",
    "            \n",
    "            if is_song and is_sixteen:\n",
    "                # replace bracketed text with a space and store to cleaned list\n",
    "                cleaned_lyrics[artist][song] = re.sub('\\[.*?\\]', ' ', all_song_lyrics[artist][song])\n",
    "                # alphanumerics\n",
    "                cleaned_lyrics[artist][song] = re.sub('\\w*\\d\\w*', ' ', cleaned_lyrics[artist][song])\n",
    "                # remove punctuation and capitalization\n",
    "                cleaned_lyrics[artist][song] = re.sub('[%s]' % re.escape(string.punctuation), '', cleaned_lyrics[artist][song].lower())\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cast to DF \n",
    "\"\"\"\n",
    "# Collect all lyrics in DataFrame\n",
    "song_dfs = []\n",
    "for artist in cleaned_lyrics.keys():\n",
    "    df = pd.DataFrame([[artist, item[0], item[1]] for item in cleaned_lyrics[artist].items()], columns=['artist', 'song', 'lyrics'])\n",
    "    song_dfs.append(df)\n",
    "all_lyrics_df = pd.concat(song_dfs)\n",
    "\n",
    "# Remove ~500 non-english songs (they seriously obfuscate our later metrics)\n",
    "all_lyrics_df['language'] = all_lyrics_df['lyrics'].apply(lambda x: detect_langs(x))\n",
    "# Ensure English is detected and remove anything with possible Korean text, our largest source of foreign data\n",
    "all_lyrics_df = all_lyrics_df[all_lyrics_df['language'].apply(lambda x: 'en' in ''.join(list(map(str, x))))]\n",
    "all_lyrics_df = all_lyrics_df[all_lyrics_df['language'].apply(lambda x:'ko' not in ''.join(list(map(str, x))))]\n",
    "\n",
    "# Remove manually identified foreign artists not caught by above \n",
    "drop_artists = ['Bang Yong-guk', 'Beenzino', 'Bigg D', 'Chingo Bling', 'Christopher Martin', 'Crucial Star', 'Davido', 'Hanhae', 'Iyanya', 'Jack Parow', 'Jay Park', \"K'naan\", 'K-OS', 'KOHH', 'Loon', 'Olamide', 'Phyno', 'Sarkodie', 'Sean Paul', 'Sik-K', 'Suga', 'Verbal Jint', 'Yama Buddha', 'Yhaunai Takiyal', 'Yo Yo Honey Singh', 'Yoon Mi-rae', 'Zico', 'E-Sens', 'The Quiett', 'Woo Won Jae', 'G-Dragon', 'Heize', 'Zeebra', 'Badshah', 'Sjava']\n",
    "all_lyrics_df = all_lyrics_df[all_lyrics_df['artist'].apply(lambda x: x not in drop_artists)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taxis Information\n",
    "Build out a DataFrame tracking total and average words, lines and unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxis_df = all_lyrics_df.copy()\n",
    "\n",
    "# Words, lines, word density\n",
    "taxis_df['lines'] = taxis_df['lyrics'].apply(lambda x: x.count('\\n') + 1)\n",
    "taxis_df['words'] = taxis_df['lyrics'].apply(lambda x: len(x))\n",
    "taxis_df['words_per_line'] = taxis_df['words'] / taxis_df['lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                               lines         words  words_per_line\nartist                                                            \nKung Fu Vampire            23.000000   1248.000000       54.260870\nQwel                       45.000000   2307.333333       49.611522\nRay Cash                   66.571429   2539.571429       48.340395\nSerius Jones              407.500000  19088.500000       46.930763\nCrime Boss                 24.000000   1124.000000       46.833333\nPolo G                     70.000000   3270.000000       46.714286\nD-Loc                      64.000000   2989.000000       45.838152\nPercee P                   61.428571   2621.857143       45.564001\nTroy Ave                   35.000000   1572.000000       44.914286\nMontana of 300             90.333333   3856.166667       44.669456\nTonedeff                   69.083333   3098.000000       44.167266\nStunna 4 Vegas             84.000000   3697.000000       44.011905\nAkrobatik                  82.314286   3594.457143       43.690459\nLil' Wil                  106.000000   4626.000000       43.641509\nSev Statik                 56.000000   2437.500000       43.609370\nAesop Rock                 83.400000   3557.000000       43.526986\nLil Twist                  50.000000   1843.818182       43.177803\nDiabolic                   78.814815   3329.925926       42.640363\nMr. Muthafuckin' eXquire   22.000000    937.000000       42.590909\nRalo                       65.000000   2763.000000       42.507692",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lines</th>\n      <th>words</th>\n      <th>words_per_line</th>\n    </tr>\n    <tr>\n      <th>artist</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Kung Fu Vampire</th>\n      <td>23.000000</td>\n      <td>1248.000000</td>\n      <td>54.260870</td>\n    </tr>\n    <tr>\n      <th>Qwel</th>\n      <td>45.000000</td>\n      <td>2307.333333</td>\n      <td>49.611522</td>\n    </tr>\n    <tr>\n      <th>Ray Cash</th>\n      <td>66.571429</td>\n      <td>2539.571429</td>\n      <td>48.340395</td>\n    </tr>\n    <tr>\n      <th>Serius Jones</th>\n      <td>407.500000</td>\n      <td>19088.500000</td>\n      <td>46.930763</td>\n    </tr>\n    <tr>\n      <th>Crime Boss</th>\n      <td>24.000000</td>\n      <td>1124.000000</td>\n      <td>46.833333</td>\n    </tr>\n    <tr>\n      <th>Polo G</th>\n      <td>70.000000</td>\n      <td>3270.000000</td>\n      <td>46.714286</td>\n    </tr>\n    <tr>\n      <th>D-Loc</th>\n      <td>64.000000</td>\n      <td>2989.000000</td>\n      <td>45.838152</td>\n    </tr>\n    <tr>\n      <th>Percee P</th>\n      <td>61.428571</td>\n      <td>2621.857143</td>\n      <td>45.564001</td>\n    </tr>\n    <tr>\n      <th>Troy Ave</th>\n      <td>35.000000</td>\n      <td>1572.000000</td>\n      <td>44.914286</td>\n    </tr>\n    <tr>\n      <th>Montana of 300</th>\n      <td>90.333333</td>\n      <td>3856.166667</td>\n      <td>44.669456</td>\n    </tr>\n    <tr>\n      <th>Tonedeff</th>\n      <td>69.083333</td>\n      <td>3098.000000</td>\n      <td>44.167266</td>\n    </tr>\n    <tr>\n      <th>Stunna 4 Vegas</th>\n      <td>84.000000</td>\n      <td>3697.000000</td>\n      <td>44.011905</td>\n    </tr>\n    <tr>\n      <th>Akrobatik</th>\n      <td>82.314286</td>\n      <td>3594.457143</td>\n      <td>43.690459</td>\n    </tr>\n    <tr>\n      <th>Lil' Wil</th>\n      <td>106.000000</td>\n      <td>4626.000000</td>\n      <td>43.641509</td>\n    </tr>\n    <tr>\n      <th>Sev Statik</th>\n      <td>56.000000</td>\n      <td>2437.500000</td>\n      <td>43.609370</td>\n    </tr>\n    <tr>\n      <th>Aesop Rock</th>\n      <td>83.400000</td>\n      <td>3557.000000</td>\n      <td>43.526986</td>\n    </tr>\n    <tr>\n      <th>Lil Twist</th>\n      <td>50.000000</td>\n      <td>1843.818182</td>\n      <td>43.177803</td>\n    </tr>\n    <tr>\n      <th>Diabolic</th>\n      <td>78.814815</td>\n      <td>3329.925926</td>\n      <td>42.640363</td>\n    </tr>\n    <tr>\n      <th>Mr. Muthafuckin' eXquire</th>\n      <td>22.000000</td>\n      <td>937.000000</td>\n      <td>42.590909</td>\n    </tr>\n    <tr>\n      <th>Ralo</th>\n      <td>65.000000</td>\n      <td>2763.000000</td>\n      <td>42.507692</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "source": [
    "# Highest word density artists\n",
    "taxis_df.groupby('artist').mean('words_per_line').sort_values('words_per_line', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique words\n",
    "# split into words, remove newlines, remove spaces and empty entries\n",
    "split_lyrics = taxis_df['lyrics'].apply(lambda x: re.sub('\\\\n', ' ', x)).apply(lambda x: x.split(' ')).apply(lambda x: [y for y in x if (y!='' and y!=' ')])\n",
    "# convert each list to a set to count unique words\n",
    "taxis_df['unique_words'] = [len(set(lyrics)) for lyrics in split_lyrics]\n",
    "# proportion of unique words (unique/total)\n",
    "taxis_df['unique_word_rate'] = taxis_df['unique_words'] / taxis_df['words']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                              lines        words  words_per_line  \\\nartist                                                             \nMr. Muthafuckin' eXquire  22.000000   937.000000       42.590909   \nKung Fu Vampire           23.000000  1248.000000       54.260870   \nLord Jamar                29.000000   914.000000       31.517241   \nEarl Sweatshirt           36.909091  1428.681818       38.492315   \nTerrace Martin            42.250000  1342.250000       31.520532   \nJuicy J                   37.500000  1426.500000       38.046586   \nTroy Ave                  35.000000  1572.000000       44.914286   \nTakeoff                   23.000000   574.000000       24.956522   \nTermanology               62.000000  1780.000000       28.709677   \nCrime Boss                24.000000  1124.000000       46.833333   \nMF Doom                   55.625000  1883.312500       35.061997   \nMadlib                    55.625000  1883.312500       35.061997   \nTha Chill                 22.500000   798.000000       37.524160   \nVado                      61.000000  2019.500000       33.074892   \nBuckwild                  52.000000  1257.000000       24.500565   \nRoc Marciano              65.636364  2101.363636       32.707762   \nWillie the Kid            61.333333  2069.666667       33.252432   \nNapoleon                  63.210526  2020.368421       32.402350   \nCount Bass D              39.818182  1203.636364       30.448073   \nMC Paul Barman            66.222222  2378.555556       36.240591   \n\n                          unique_words  unique_word_rate  \nartist                                                    \nMr. Muthafuckin' eXquire    133.000000          0.141942  \nKung Fu Vampire             158.000000          0.126603  \nLord Jamar                  115.000000          0.125821  \nEarl Sweatshirt             171.500000          0.124831  \nTerrace Martin              158.250000          0.124210  \nJuicy J                     177.000000          0.124080  \nTroy Ave                    195.000000          0.124046  \nTakeoff                      71.000000          0.123693  \nTermanology                 220.000000          0.123596  \nCrime Boss                  138.000000          0.122776  \nMF Doom                     228.250000          0.122641  \nMadlib                      228.250000          0.122641  \nTha Chill                    97.500000          0.122436  \nVado                        244.500000          0.121215  \nBuckwild                    150.000000          0.119171  \nRoc Marciano                245.363636          0.119054  \nWillie the Kid              246.666667          0.118968  \nNapoleon                    236.789474          0.118799  \nCount Bass D                135.727273          0.118172  \nMC Paul Barman              275.166667          0.117874  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lines</th>\n      <th>words</th>\n      <th>words_per_line</th>\n      <th>unique_words</th>\n      <th>unique_word_rate</th>\n    </tr>\n    <tr>\n      <th>artist</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Mr. Muthafuckin' eXquire</th>\n      <td>22.000000</td>\n      <td>937.000000</td>\n      <td>42.590909</td>\n      <td>133.000000</td>\n      <td>0.141942</td>\n    </tr>\n    <tr>\n      <th>Kung Fu Vampire</th>\n      <td>23.000000</td>\n      <td>1248.000000</td>\n      <td>54.260870</td>\n      <td>158.000000</td>\n      <td>0.126603</td>\n    </tr>\n    <tr>\n      <th>Lord Jamar</th>\n      <td>29.000000</td>\n      <td>914.000000</td>\n      <td>31.517241</td>\n      <td>115.000000</td>\n      <td>0.125821</td>\n    </tr>\n    <tr>\n      <th>Earl Sweatshirt</th>\n      <td>36.909091</td>\n      <td>1428.681818</td>\n      <td>38.492315</td>\n      <td>171.500000</td>\n      <td>0.124831</td>\n    </tr>\n    <tr>\n      <th>Terrace Martin</th>\n      <td>42.250000</td>\n      <td>1342.250000</td>\n      <td>31.520532</td>\n      <td>158.250000</td>\n      <td>0.124210</td>\n    </tr>\n    <tr>\n      <th>Juicy J</th>\n      <td>37.500000</td>\n      <td>1426.500000</td>\n      <td>38.046586</td>\n      <td>177.000000</td>\n      <td>0.124080</td>\n    </tr>\n    <tr>\n      <th>Troy Ave</th>\n      <td>35.000000</td>\n      <td>1572.000000</td>\n      <td>44.914286</td>\n      <td>195.000000</td>\n      <td>0.124046</td>\n    </tr>\n    <tr>\n      <th>Takeoff</th>\n      <td>23.000000</td>\n      <td>574.000000</td>\n      <td>24.956522</td>\n      <td>71.000000</td>\n      <td>0.123693</td>\n    </tr>\n    <tr>\n      <th>Termanology</th>\n      <td>62.000000</td>\n      <td>1780.000000</td>\n      <td>28.709677</td>\n      <td>220.000000</td>\n      <td>0.123596</td>\n    </tr>\n    <tr>\n      <th>Crime Boss</th>\n      <td>24.000000</td>\n      <td>1124.000000</td>\n      <td>46.833333</td>\n      <td>138.000000</td>\n      <td>0.122776</td>\n    </tr>\n    <tr>\n      <th>MF Doom</th>\n      <td>55.625000</td>\n      <td>1883.312500</td>\n      <td>35.061997</td>\n      <td>228.250000</td>\n      <td>0.122641</td>\n    </tr>\n    <tr>\n      <th>Madlib</th>\n      <td>55.625000</td>\n      <td>1883.312500</td>\n      <td>35.061997</td>\n      <td>228.250000</td>\n      <td>0.122641</td>\n    </tr>\n    <tr>\n      <th>Tha Chill</th>\n      <td>22.500000</td>\n      <td>798.000000</td>\n      <td>37.524160</td>\n      <td>97.500000</td>\n      <td>0.122436</td>\n    </tr>\n    <tr>\n      <th>Vado</th>\n      <td>61.000000</td>\n      <td>2019.500000</td>\n      <td>33.074892</td>\n      <td>244.500000</td>\n      <td>0.121215</td>\n    </tr>\n    <tr>\n      <th>Buckwild</th>\n      <td>52.000000</td>\n      <td>1257.000000</td>\n      <td>24.500565</td>\n      <td>150.000000</td>\n      <td>0.119171</td>\n    </tr>\n    <tr>\n      <th>Roc Marciano</th>\n      <td>65.636364</td>\n      <td>2101.363636</td>\n      <td>32.707762</td>\n      <td>245.363636</td>\n      <td>0.119054</td>\n    </tr>\n    <tr>\n      <th>Willie the Kid</th>\n      <td>61.333333</td>\n      <td>2069.666667</td>\n      <td>33.252432</td>\n      <td>246.666667</td>\n      <td>0.118968</td>\n    </tr>\n    <tr>\n      <th>Napoleon</th>\n      <td>63.210526</td>\n      <td>2020.368421</td>\n      <td>32.402350</td>\n      <td>236.789474</td>\n      <td>0.118799</td>\n    </tr>\n    <tr>\n      <th>Count Bass D</th>\n      <td>39.818182</td>\n      <td>1203.636364</td>\n      <td>30.448073</td>\n      <td>135.727273</td>\n      <td>0.118172</td>\n    </tr>\n    <tr>\n      <th>MC Paul Barman</th>\n      <td>66.222222</td>\n      <td>2378.555556</td>\n      <td>36.240591</td>\n      <td>275.166667</td>\n      <td>0.117874</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "# Highest unique word rate artists\n",
    "taxis_df.groupby('artist').mean().sort_values('unique_word_rate', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prosody Information \n",
    "Build out a DataFrame tracking syllables (total and averages by line and word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "prosody_df = all_lyrics_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note, this takes a couple minutes because there are over 40 million words to estimate\n",
    "prosody_df['syllables'] = split_lyrics.apply(lambda x: sum([syllables.estimate(word) for word in x]))\n",
    "# Average syllables per line\n",
    "prosody_df['syllables_per_line'] = prosody_df['syllables'] / taxis_df['lines']\n",
    "# Average syllables per word \n",
    "prosody_df['syllables_per_word'] = prosody_df['syllables'] / taxis_df['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nuse Phyme to calculate rhyme statistics\\n'"
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": [
    "\"\"\"\n",
    "use Phyme to calculate rhyme statistics\n",
    "**for future expansion\n",
    "\"\"\"\n",
    "# from Phyme import Phyme\n",
    "# ph = Phyme()\n",
    "# ph.get_..._rhymes(word, num_sylls=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                              syllables  syllables_per_line  \\\nartist                                                        \nBia                          339.000000            5.380952   \nKitty                        481.000000            5.722771   \nJipsta                       481.000000            5.722771   \nSilla                        432.333333            5.995531   \nMilo                         391.500000            6.653482   \nAfrika Bambaataa             706.000000            6.059524   \nNicky da B                   795.000000            8.548387   \nRoger Troutman               625.333333            7.002242   \nUnk                          390.000000            5.416667   \nBahamadia                    648.000000           10.503152   \nBlack Thought               1480.000000           11.570427   \nImran Khan                   638.000000            8.985915   \n2Mex                         601.750000            8.142407   \nBurna Boy                    423.333333            7.371511   \nTimaya                       647.444444            8.158618   \nYoungBoy Never Broke Again   807.000000           10.905405   \nKwayzar                      510.000000            7.846154   \nT La Rock                    460.000000            7.540984   \nKung Fu Vampire              346.000000           15.043478   \nAfroman                      793.250000            7.860274   \n\n                            syllables_per_word  \nartist                                          \nBia                                   0.307065  \nKitty                                 0.295019  \nJipsta                                0.295019  \nSilla                                 0.294874  \nMilo                                  0.285703  \nAfrika Bambaataa                      0.285356  \nNicky da B                            0.285151  \nRoger Troutman                        0.284763  \nUnk                                   0.284719  \nBahamadia                             0.282540  \nBlack Thought                         0.282270  \nImran Khan                            0.282051  \n2Mex                                  0.281227  \nBurna Boy                             0.281177  \nTimaya                                0.281124  \nYoungBoy Never Broke Again            0.280696  \nKwayzar                               0.278384  \nT La Rock                             0.278282  \nKung Fu Vampire                       0.277244  \nAfroman                               0.276814  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>syllables</th>\n      <th>syllables_per_line</th>\n      <th>syllables_per_word</th>\n    </tr>\n    <tr>\n      <th>artist</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Bia</th>\n      <td>339.000000</td>\n      <td>5.380952</td>\n      <td>0.307065</td>\n    </tr>\n    <tr>\n      <th>Kitty</th>\n      <td>481.000000</td>\n      <td>5.722771</td>\n      <td>0.295019</td>\n    </tr>\n    <tr>\n      <th>Jipsta</th>\n      <td>481.000000</td>\n      <td>5.722771</td>\n      <td>0.295019</td>\n    </tr>\n    <tr>\n      <th>Silla</th>\n      <td>432.333333</td>\n      <td>5.995531</td>\n      <td>0.294874</td>\n    </tr>\n    <tr>\n      <th>Milo</th>\n      <td>391.500000</td>\n      <td>6.653482</td>\n      <td>0.285703</td>\n    </tr>\n    <tr>\n      <th>Afrika Bambaataa</th>\n      <td>706.000000</td>\n      <td>6.059524</td>\n      <td>0.285356</td>\n    </tr>\n    <tr>\n      <th>Nicky da B</th>\n      <td>795.000000</td>\n      <td>8.548387</td>\n      <td>0.285151</td>\n    </tr>\n    <tr>\n      <th>Roger Troutman</th>\n      <td>625.333333</td>\n      <td>7.002242</td>\n      <td>0.284763</td>\n    </tr>\n    <tr>\n      <th>Unk</th>\n      <td>390.000000</td>\n      <td>5.416667</td>\n      <td>0.284719</td>\n    </tr>\n    <tr>\n      <th>Bahamadia</th>\n      <td>648.000000</td>\n      <td>10.503152</td>\n      <td>0.282540</td>\n    </tr>\n    <tr>\n      <th>Black Thought</th>\n      <td>1480.000000</td>\n      <td>11.570427</td>\n      <td>0.282270</td>\n    </tr>\n    <tr>\n      <th>Imran Khan</th>\n      <td>638.000000</td>\n      <td>8.985915</td>\n      <td>0.282051</td>\n    </tr>\n    <tr>\n      <th>2Mex</th>\n      <td>601.750000</td>\n      <td>8.142407</td>\n      <td>0.281227</td>\n    </tr>\n    <tr>\n      <th>Burna Boy</th>\n      <td>423.333333</td>\n      <td>7.371511</td>\n      <td>0.281177</td>\n    </tr>\n    <tr>\n      <th>Timaya</th>\n      <td>647.444444</td>\n      <td>8.158618</td>\n      <td>0.281124</td>\n    </tr>\n    <tr>\n      <th>YoungBoy Never Broke Again</th>\n      <td>807.000000</td>\n      <td>10.905405</td>\n      <td>0.280696</td>\n    </tr>\n    <tr>\n      <th>Kwayzar</th>\n      <td>510.000000</td>\n      <td>7.846154</td>\n      <td>0.278384</td>\n    </tr>\n    <tr>\n      <th>T La Rock</th>\n      <td>460.000000</td>\n      <td>7.540984</td>\n      <td>0.278282</td>\n    </tr>\n    <tr>\n      <th>Kung Fu Vampire</th>\n      <td>346.000000</td>\n      <td>15.043478</td>\n      <td>0.277244</td>\n    </tr>\n    <tr>\n      <th>Afroman</th>\n      <td>793.250000</td>\n      <td>7.860274</td>\n      <td>0.276814</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "source": [
    "prosody_df.groupby('artist').mean().sort_values('syllables_per_word', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the processed DataFrames for quick access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/all_lyrics_df.pkl', 'wb') as f:\n",
    "    pkl.dump(all_lyrics_df, f)\n",
    "\n",
    "with open('data/taxis_df.pkl', 'wb') as f:\n",
    "    pkl.dump(taxis_df, f)\n",
    "\n",
    "with open('data/prosody_df.pkl', 'wb') as f:\n",
    "    pkl.dump(prosody_df, f)"
   ]
  }
 ]
}